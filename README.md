ğŸ‘¤ Author

William Akitikori

Adaptable Technical Support Specialist & Aspiring Engineer
Passionate about data engineering and userâ€‘focused innovation.



# ğŸ› ï¸ Lightweight ETL Pipeline for Multiâ€‘Source Sales Data

## ğŸ“Œ Overview
This project implements a **lightweight ETL (Extract, Transform, Load) pipeline** that consolidates sales and transaction data from **Shopify**, **Square**, **Dojo**, and **Tide** into a single, clean dataset.  
The unified dataset is designed for **future analytics, reporting, or dashboard integration**, providing a reliable foundation for business decisionâ€‘making.

---

## ğŸ’¡ Why This Project Matters
From a data engineering perspective, this project demonstrates:
- Realâ€‘world **data wrangling** from multiple, inconsistent sources
- **Pipeline design** and **data standardisation** techniques
- Foundations of **automation** and **reproducibility**
- Creation of a **businessâ€‘relevant asset** for *Awe Kids*

---

## ğŸ“Š Data Sources
The pipeline processes CSV exports from:
1. **Shopify** â€“ Orders/Sales report (October 2024)
2. **Square** â€“ Pointâ€‘ofâ€‘sale transactions (markets & trade supply meetings)
3. **Dojo** â€“ Card terminal transactions
4. **Tide** â€“ Bank statement or transaction export of income

> Each dataset varies in structure and schema. The pipeline standardises and unifies them into a consistent format.

---

## âš™ï¸ Tools & Technologies
- **Python**: `pandas`, `datetime`
- **Jupyter Notebook** / **Google Colab**
- **Git & GitHub** for version control

---

## ğŸ”„ ETL Process

### 1. **Extract**
- Load CSV exports from each platform into pandas DataFrames
- Handle different file encodings and delimiters

### 2. **Transform**
- Standardise column names and data types
- Parse and normalise date formats
- Map transaction types and payment methods to a common schema
- Remove duplicates and handle missing values
- Merge datasets into a single DataFrame

### 3. **Load**
- Export the cleaned dataset to:
  - **CSV** for portability

---

## ğŸš€ Getting Started

### Clone the Repository
```bash
git clone https://github.com/yourusername/etl-sales-pipeline.git
cd etl-sales-pipeline

### Install Dependencies
pip install -r requirements.txt

### Run the Pipeline
python src/main.py

---

ğŸ”® Future Enhancements
- Automate data ingestion from APIs instead of manual CSV exports
- Integrate with Apache Airflow for scheduling
- Deploy to cloud storage (AWS S3 / Azure Blob)
- Add unit tests for data validation
